Abstractive Text Summarization using LSTM

 The explosion of big data and technology has created a huge
 amount of information that it becomes difficult for individuals to pro
cess the information efficiently. It would be impracticable to have large
 texts, read and decide on relevance manually, since much of it might not
 be about the topic a reader is looking for. Regarding this, text summa
rization is one of the most important jobs in Natural Language Process
ing (NLP). It helps in summarizing gigantic chunks of information into
 short pieces while retaining much of the critical details. This project im
plements a deep learning-based abstractive text summarization model.
 We further used the sequence-to-sequence model architecture, an en
coder which reads the source text and focuses on key information, and a
 decoder which then generates a coherent, condensed summary. A Long
 Short Term Memory (LSTM) network is utilized in both the encoder and
 decoder for capturing and retaining dependencies in the text over long
 ranges. By training the model and fine tuning it with a large and diverse
 dataset, we were able to generate a reasonably good summary for a given
 text.

 Results :

 Original summary: start half train accidents last years due derailment end
 
 Predicted summary: start railways get highest train due people end
